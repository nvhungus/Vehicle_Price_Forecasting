{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4098d9dc-2742-4718-a5c4-d74544f0aaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1422dc8-4fe2-4c8e-a2bf-57867945b7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    \"\"\"\n",
    "    A class implementing a Linear Regression model with Gradient Descent and L2 Regularization.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=0.01, epochs=2000, lambda_l2=0.1, tolerance=1e-7, patience=100):\n",
    "        \"\"\"\n",
    "        Initializes the model with hyperparameters.\n",
    "        \n",
    "        Args:\n",
    "            learning_rate (float): The step size for gradient descent.\n",
    "            epochs (int): The maximum number of iterations over the dataset.\n",
    "            lambda_l2 (float): The regularization parameter for L2 regularization.\n",
    "            tolerance (float): The minimum improvement in validation loss to be considered progress.\n",
    "            patience (int): The number of epochs to wait for improvement before early stopping.\n",
    "        \"\"\"\n",
    "        self.lr = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.lambda_l2 = lambda_l2\n",
    "        self.tolerance = tolerance\n",
    "        self.patience = patience\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.history = {'train_loss': [], 'val_loss': []}\n",
    "\n",
    "    def fit(self, X_train, y_train, X_val, y_val):\n",
    "        \"\"\"\n",
    "        Fits the linear regression model to the training data using gradient descent.\n",
    "        Implements early stopping based on validation loss.\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X_train.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        best_val_loss = float('inf')\n",
    "        no_improve_epochs = 0\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            # Calculate predictions on the training set\n",
    "            y_pred_train = np.dot(X_train, self.weights) + self.bias\n",
    "            \n",
    "            # Calculate gradients for weights (dw) and bias (db) with L2 regularization\n",
    "            dw = (1/n_samples) * np.dot(X_train.T, (y_pred_train - y_train)) + self.lambda_l2 * self.weights\n",
    "            db = (1/n_samples) * np.sum(y_pred_train - y_train)\n",
    "            \n",
    "            # Update weights and bias using gradient descent\n",
    "            self.weights -= self.lr * dw\n",
    "            self.bias -= self.lr * db\n",
    "            \n",
    "            # Calculate and store training and validation loss\n",
    "            train_loss = self.mean_squared_error(y_train, y_pred_train)\n",
    "            y_pred_val = self.predict(X_val)\n",
    "            val_loss = self.mean_squared_error(y_val, y_pred_val)\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            \n",
    "            if (epoch + 1) % 1000 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{self.epochs} | Train Loss: {train_loss:.4f} | Validation Loss: {val_loss:.4f}\")\n",
    "            \n",
    "            # Early stopping check\n",
    "            if val_loss < best_val_loss - self.tolerance:\n",
    "                best_val_loss = val_loss\n",
    "                no_improve_epochs = 0\n",
    "            else:\n",
    "                no_improve_epochs += 1\n",
    "            \n",
    "            if no_improve_epochs >= self.patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}.\")\n",
    "                break\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Makes predictions using the trained linear model.\n",
    "        \"\"\"\n",
    "        return np.dot(X, self.weights) + self.bias\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_squared_error(y_true, y_pred):\n",
    "        \"\"\"Calculates the Mean Squared Error metric.\"\"\"\n",
    "        return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_absolute_error(y_true, y_pred):\n",
    "        \"\"\"Calculates the Mean Absolute Error metric.\"\"\"\n",
    "        return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "    @staticmethod\n",
    "    def r2_score(y_true, y_pred):\n",
    "        \"\"\"Calculates the R-squared (coefficient of determination) metric.\"\"\"\n",
    "        ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "        ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "        if ss_tot == 0: return 1.0  # Handle case where y_true is constant\n",
    "        return 1 - (ss_res / ss_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54400b2-115f-44f9-96b7-ab718d595a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarPricePredictor:\n",
    "    \"\"\"\n",
    "    A class that encapsulates the entire pipeline from preprocessing to car price prediction.\n",
    "    This class handles data cleaning, feature engineering, training, and prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, top_n_makes=12, current_year=2024):\n",
    "        \"\"\"\n",
    "        Initializes the predictor pipeline.\n",
    "        \n",
    "        Args:\n",
    "            top_n_makes (int): The number of most frequent car makes to keep as separate features.\n",
    "            current_year (int): The current year, used to calculate car age.\n",
    "        \"\"\"\n",
    "        self.current_year = current_year\n",
    "        self.top_n_makes = top_n_makes\n",
    "        self.model = None\n",
    "        self.state = {} # Dictionary to store all learned parameters (medians, scalers, etc.)\n",
    "\n",
    "    def _preprocess(self, df, fit=False):\n",
    "        \"\"\"\n",
    "        Internal method to handle all data preprocessing steps.\n",
    "        If fit=True, it learns and stores preprocessing parameters in self.state.\n",
    "        Otherwise, it uses the stored parameters to transform the data.\n",
    "        \"\"\"\n",
    "        # Select a subset of columns to use\n",
    "        cols_to_use = ['Year', 'Kilometer', 'Fuel Type', 'Transmission', 'Owner', 'Make', \n",
    "                       'Engine', 'Max Power', 'Max Torque', 'Seating Capacity']\n",
    "        if 'Price' in df.columns:\n",
    "            cols_to_use.append('Price')\n",
    "        data = df[[col for col in cols_to_use if col in df.columns]].copy()\n",
    "\n",
    "        # Feature Engineering: Create 'Age' from 'Year'\n",
    "        if 'Year' in data.columns: data['Age'] = self.current_year - data['Year']\n",
    "\n",
    "        # Helper function to extract numeric values from string columns\n",
    "        def extract_numeric(text):\n",
    "            if pd.isna(text): return np.nan\n",
    "            match = re.search(r'(\\d+\\.?\\d*)', str(text).lower())\n",
    "            return float(match.group(1)) if match else np.nan\n",
    "        \n",
    "        # Apply numeric extraction to relevant columns\n",
    "        for col in ['Engine', 'Max Power', 'Max Torque']:\n",
    "            if col in data.columns: data[col] = data[col].apply(extract_numeric)\n",
    "\n",
    "        # Impute missing numeric values with the median\n",
    "        numeric_cols = ['Kilometer', 'Engine', 'Max Power', 'Max Torque', 'Seating Capacity', 'Age']\n",
    "        if fit: self.state['medians'] = {}\n",
    "        for col in numeric_cols:\n",
    "            if col in data.columns:\n",
    "                if fit: self.state['medians'][col] = data[col].median()\n",
    "                data[col] = data[col].fillna(self.state.get(col, data[col].median()))\n",
    "\n",
    "        # Map 'Owner' column to numerical values\n",
    "        if 'Owner' in data.columns:\n",
    "            if fit: self.state['owner_map'] = {'First': 1, 'Second': 2, 'Third': 3, 'Fourth & Above': 4, 'Test Drive Car': 0}\n",
    "            data['Owner'] = data['Owner'].map(self.state.get('owner_map', {})).fillna(0)\n",
    "\n",
    "        # Handle 'Make' by keeping top N and grouping others into 'Other'\n",
    "        if 'Make' in data.columns:\n",
    "            if fit: self.state['top_makes'] = data['Make'].value_counts().nlargest(self.top_n_makes).index.tolist()\n",
    "            data['Make'] = data['Make'].apply(lambda x: x if x in self.state.get('top_makes', []) else 'Other')\n",
    "\n",
    "        # Perform one-hot encoding for categorical features\n",
    "        data = pd.get_dummies(data, columns=['Make', 'Fuel Type', 'Transmission'], drop_first=True)\n",
    "\n",
    "        # Feature Engineering: Create polynomial features\n",
    "        if 'Age' in data.columns: data['Age_Squared'] = data['Age']**2\n",
    "        if 'Kilometer' in data.columns: data['Kilometer_Squared'] = data['Kilometer']**2\n",
    "            \n",
    "        # Store the final list of feature columns during fitting\n",
    "        if fit:\n",
    "            self.state['final_features'] = [c for c in data.columns if c not in ['Year', 'Price']]\n",
    "        \n",
    "        # Reindex columns to ensure consistency between train and test sets\n",
    "        data = data.reindex(columns=self.state.get('final_features', []), fill_value=0)\n",
    "        return data\n",
    "\n",
    "    def fit(self, df_train, learning_rate=0.001, epochs=15000, lambda_l2=0.01, patience=800):\n",
    "        \"\"\"\n",
    "        Executes the full training pipeline: outlier removal, preprocessing, splitting, scaling, and model training.\n",
    "        \"\"\"\n",
    "        print(\"--- Starting Training Pipeline ---\")\n",
    "        # 1. Remove outliers from the target variable 'Price'\n",
    "        q_low, q_hi = df_train[\"Price\"].quantile(0.01), df_train[\"Price\"].quantile(0.99)\n",
    "        data_cleaned = df_train[(df_train[\"Price\"] >= q_low) & (df_train[\"Price\"] <= q_hi)].copy()\n",
    "        print(f\"Removed outliers: {len(data_cleaned)}/{len(df_train)} samples remaining.\")\n",
    "        \n",
    "        # 2. Preprocess data and learn transformation parameters (fit=True)\n",
    "        X = self._preprocess(data_cleaned, fit=True)\n",
    "        # Apply log transformation to the target variable to handle skewed distribution\n",
    "        y = np.log1p(data_cleaned['Price'])\n",
    "\n",
    "        # 3. Split data into training and validation sets\n",
    "        np.random.seed(42)\n",
    "        indices = np.random.permutation(X.shape[0])\n",
    "        val_size = int(X.shape[0] * 0.2)\n",
    "        val_indices, train_indices = indices[:val_size], indices[val_size:]\n",
    "        X_train, X_val = X.iloc[train_indices], X.iloc[val_indices]\n",
    "        y_train, y_val = y.iloc[train_indices], y.iloc[val_indices]\n",
    "\n",
    "        # 4. Standardize data: learn scaler on training data and transform both sets\n",
    "        self.state['scaler_mean'] = X_train.mean(axis=0)\n",
    "        self.state['scaler_std'] = X_train.std(axis=0)\n",
    "        self.state['scaler_std'][self.state['scaler_std'] == 0] = 1.0 # Avoid division by zero\n",
    "        X_train_scaled = (X_train - self.state['scaler_mean']) / self.state['scaler_std']\n",
    "        X_val_scaled = (X_val - self.state['scaler_mean']) / self.state['scaler_std']\n",
    "        \n",
    "        # 5. Train the Linear Regression model\n",
    "        print(\"\\nTraining the Linear Regression model...\")\n",
    "        self.model = LinearRegression(learning_rate=learning_rate, epochs=epochs, lambda_l2=lambda_l2, patience=patience)\n",
    "        self.model.fit(X_train_scaled.values, y_train.values, X_val_scaled.values, y_val.values)\n",
    "        print(\"--- Training complete ---\")\n",
    "        \n",
    "        # Evaluate on the full (cleaned) training set after training is done\n",
    "        print(\"\\n--- Evaluating on the full training set ---\")\n",
    "        self.evaluate(data_cleaned, dataset_name=\"Train\")\n",
    "\n",
    "    def predict(self, df_new):\n",
    "        \"\"\"\n",
    "        Makes price predictions on new, unseen data.\n",
    "        \"\"\"\n",
    "        if not self.model: raise RuntimeError(\"Model has not been trained. Please call .fit() first.\")\n",
    "        # Preprocess the new data using the saved state from training (fit=False)\n",
    "        X_new = self._preprocess(df_new, fit=False)\n",
    "        # Scale the new data using the saved scaler\n",
    "        X_new_scaled = (X_new - self.state['scaler_mean']) / self.state['scaler_std']\n",
    "        # Predict on the log-transformed scale\n",
    "        y_pred_log = self.model.predict(X_new_scaled.values)\n",
    "        # Revert the log transformation to get the actual price prediction\n",
    "        y_pred_real = np.expm1(y_pred_log)\n",
    "        return y_pred_real\n",
    "\n",
    "    def evaluate(self, df, dataset_name=\"Test\"):\n",
    "        \"\"\"\n",
    "        Evaluates the model on a given dataframe and prints performance metrics.\n",
    "        The `dataset_name` parameter is used for clear print statements.\n",
    "        \"\"\"\n",
    "        if 'Price' not in df.columns:\n",
    "            print(f\"Error: 'Price' column not found in the {dataset_name} dataset for evaluation.\")\n",
    "            return None, None\n",
    "\n",
    "        y_true = df['Price']\n",
    "        y_pred = self.predict(df)\n",
    "        \n",
    "        # Calculate metrics on the real price scale\n",
    "        mse = self.model.mean_squared_error(y_true, y_pred)\n",
    "        mae = self.model.mean_absolute_error(y_true, y_pred)\n",
    "        r2 = self.model.r2_score(y_true, y_pred)\n",
    "        \n",
    "        print(f\"Metrics for {dataset_name} set:\")\n",
    "        print(f\"  R^2 Score (Real Scale): {r2:.4f}\")\n",
    "        print(f\"  Mean Squared Error (MSE): {mse:,.0f}\")\n",
    "        print(f\"  Mean Absolute Error (MAE): {mae:,.0f}\")\n",
    "        \n",
    "        # Prepare results DataFrame for visualization or inspection\n",
    "        results = df[['Make', 'Model', 'Year']].copy()\n",
    "        results['Actual_Price'] = y_true\n",
    "        results['Predicted_Price'] = np.round(y_pred, 0)\n",
    "        return results, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d58a4dc-a4c1-4461-a022-272575219596",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = CarPricePredictor(top_n_makes=12)\n",
    "df_train_full = pd.read_csv('train.csv')\n",
    "\n",
    "# The .fit() method will run the entire training pipeline and print training set evaluation\n",
    "predictor.fit(df_train_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c347ab4-0dbe-47ba-be5a-d173fe9e16a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATING ON THE TEST DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df_test = pd.read_csv('test.csv')\n",
    "test_results, test_r2 = predictor.evaluate(df_test, dataset_name=\"Test\")\n",
    "\n",
    "if test_results is not None:\n",
    "    print(\"\\nPreview of the first 10 predictions on the test set:\")\n",
    "    print(test_results.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc5d323-eb5c-454b-afed-d81e613aa415",
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_results is not None:\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f521d260-1af2-49e9-ac4d-87a294d98a5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2855797-74dd-4354-babf-26f70b235b70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d4c738-259b-444e-a3ad-28f0b4b42498",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019aed42-cb4c-403b-8e81-d87011c60b2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
